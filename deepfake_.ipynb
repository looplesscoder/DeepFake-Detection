{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFWOCy73Gx7"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H7ayz8gKNl5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk(\"/content/drive/MyDrive/DL project\"):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "H18lrOrM5KS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib"
      ],
      "metadata": {
        "id": "OwG17iIq9iCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pylab import *\n",
        "from PIL import Image, ImageChops, ImageEnhance"
      ],
      "metadata": {
        "id": "FV2_34f7Bw4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('/content/drive/MyDrive/dataset')\n",
        "os.mkdir('/content/drive/MyDrive/dataset/real')\n",
        "os.mkdir('/content/drive/MyDrive/dataset/fake')"
      ],
      "metadata": {
        "id": "ttsH1vxUH_8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_frame_folder = '/content/drive/MyDrive/DL project/train_sample_videos'\n",
        "with open(os.path.join(train_frame_folder, 'metadata.json'), 'r') as file:\n",
        "    data = json.load(file)\n",
        "list_of_train_data = [f for f in os.listdir(train_frame_folder) if f.endswith('.mp4')]\n",
        "#detects face using linear regression and svm\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "for vid in list_of_train_data:\n",
        "    count = 0\n",
        "    cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid)) \n",
        "    frameRate = cap.get(5) #frequency at which the images are generated consectively\n",
        "    while cap.isOpened():\n",
        "        frameId = cap.get(1) #to get the current frame no.\n",
        "        ret, frame = cap.read()\n",
        "        if ret != True:\n",
        "            break\n",
        "        if frameId % ((int(frameRate)+1)*1) == 0:\n",
        "            face_rects, scores, idx = detector.run(frame, 0)\n",
        "            #retrieving the detection results \n",
        "            for i, d in enumerate(face_rects):\n",
        "                x1 = d.left()\n",
        "                y1 = d.top()\n",
        "                x2 = d.right()\n",
        "                y2 = d.bottom()\n",
        "                crop_img = frame[y1:y2, x1:x2]\n",
        "                if data[vid]['label'] == 'REAL':\n",
        "                    cv2.imwrite('/content/drive/MyDrive/dataset/real/'+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))\n",
        "                elif data[vid]['label'] == 'FAKE':\n",
        "                    cv2.imwrite('/content/drive/MyDrive/dataset/fake/ '+vid.split('.')[0]+'_'+str(count)+'.png', cv2.resize(crop_img, (128, 128)))\n",
        "                count+=1\n",
        "                "
      ],
      "metadata": {
        "id": "TgXFAVx2B3Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "QoQqV20HAU72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128, 128, 3)\n",
        "data_dir = '/content/drive/MyDrive/dataset/'\n",
        "\n",
        "real_data = [f for f in os.listdir(data_dir+'/real') if f.endswith('.png')]\n",
        "fake_data = [f for f in os.listdir(data_dir+'/fake') if f.endswith('.png')]\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "#dividing and assigning data to X and Y variables where X holds the real and fake images and Y is the 1 for real image and 0 for fake image\n",
        "for img in real_data:\n",
        "    X.append(img_to_array(load_img(data_dir+'/real/'+img)).flatten() / 255.0)\n",
        "    Y.append(1)\n",
        "for img in fake_data:\n",
        "    X.append(img_to_array(load_img(data_dir+'/fake/'+img)).flatten() / 255.0)\n",
        "    Y.append(0)\n",
        "\n",
        "Y_val_org = Y\n",
        "\n",
        "#Normalization\n",
        "X = np.array(X)\n",
        "Y = to_categorical(Y, 2) #converting vector to binary class matrix with 0 and 1 values \n",
        "\n",
        "#Reshape\n",
        "X = X.reshape(-1, 128, 128, 3)\n",
        "\n",
        "#Train-Test split\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)"
      ],
      "metadata": {
        "id": "7ofiSZYjBKJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train)\n",
        "print(X_train)"
      ],
      "metadata": {
        "id": "yCq2xAr0fow3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model #takes the input which is an image and output is segmentation class\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "#pretrained model\n",
        "#loading the weights therefore using imagenet and not including the fully connected layer\n",
        "googleNet_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "googleNet_model.trainable = True\n",
        "model = Sequential()\n",
        "model.add(googleNet_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "# binary_crossentropy function computes the cross-entropy loss \n",
        "# between true labels and predicted labels.\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oHD_PKadf8IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               min_delta=0,\n",
        "                               patience=2,\n",
        "                               verbose=0, mode='auto')\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 100\n",
        "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_val, Y_val), verbose = 1)"
      ],
      "metadata": {
        "id": "inRnfKtxgK8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CrhHWjHNjs8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))\n",
        "t = f.suptitle('Pre-trained InceptionResNetV2 Transfer Learn with Fine-Tuning & Image Augmentation Performance ', fontsize=12)\n",
        "f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "epoch_list = list(range(1,EPOCHS+1))\n",
        "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
        "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_xticks(np.arange(0, EPOCHS+1, 1))\n",
        "ax1.set_ylabel('Accuracy Value')\n",
        "ax1.set_xlabel('Epoch #')\n",
        "ax1.set_title('Accuracy')\n",
        "l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
        "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_xticks(np.arange(0, EPOCHS+1, 1))\n",
        "ax2.set_ylabel('Loss Value')\n",
        "ax2.set_xlabel('Epoch #')\n",
        "ax2.set_title('Loss')\n",
        "l2 = ax2.legend(loc=\"best\")"
      ],
      "metadata": {
        "id": "pJ-vu6rQk9JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "jW6p_Q6poIZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print('True positive = ', cm[0][0])\n",
        "    print('False positive = ', cm[0][1])\n",
        "    print('False negative = ', cm[1][0])\n",
        "    print('True negative = ', cm[1][1])\n",
        "    print('\\n')\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "    plt.ylabel('Actual label', size = 20)\n",
        "    plt.xlabel('Predicted label', size = 20)\n",
        "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.ylim([2, 0])\n",
        "    plt.show()\n",
        "\n",
        "print_confusion_matrix(Y_val_org, np.argmax(model.predict(X), axis=-1))\n"
      ],
      "metadata": {
        "id": "JXl2lVrTmI45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('deepfake-detection-model.h5')"
      ],
      "metadata": {
        "id": "jMJ0YGsGnSru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image, ImageChops, ImageEnhance\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ],
      "metadata": {
        "id": "D1id4WForxWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "id": "q7lvz8K8r9Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('deepfake-detection-model.h5')"
      ],
      "metadata": {
        "id": "edqITxzir-pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128, 128, 3)\n",
        "pr_data = []\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/DL project/test_videos')\n",
        "frameRate = cap.get(5)\n",
        "while cap.isOpened():\n",
        "    frameId = cap.get(1)\n",
        "    ret, frame = cap.read()\n",
        "    if ret != True:\n",
        "        break\n",
        "    if frameId % ((int(frameRate)+1)*1) == 0:\n",
        "        face_rects, scores, idx = detector.run(frame, 0)\n",
        "        for i, d in enumerate(face_rects):\n",
        "            x1 = d.left()\n",
        "            y1 = d.top()\n",
        "            x2 = d.right()\n",
        "            y2 = d.bottom()\n",
        "            crop_img = frame[y1:y2, x1:x2]\n",
        "            data = img_to_array(cv2.resize(crop_img, (128, 128))).flatten() / 255.0\n",
        "            data = data.reshape(-1, 128, 128, 3)\n",
        "            print(model.predict_classes(data))\n"
      ],
      "metadata": {
        "id": "jfPsBbh1sIw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "cm = sklearn.metrics.accuracy_score(Y_val_org, np.argmax(model.predict(X), axis=-1))"
      ],
      "metadata": {
        "id": "kDWOzORssZ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cm)"
      ],
      "metadata": {
        "id": "1rZyJWVds0Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install keras_efficientnets\n",
        "\n",
        "from keras_efficientnets import EfficientNetB0"
      ],
      "metadata": {
        "id": "6YlZJOq6r1GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "xZ4MIHnqrxMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"./deepfake-detection-model.h5\"\n",
        "\n",
        "h5 = h5py.File(filename,'r')\n",
        "print(h5)\n",
        "h5.close()"
      ],
      "metadata": {
        "id": "FYzGj3LKwO07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.optimizers import SGD,Adam"
      ],
      "metadata": {
        "id": "4inauLzowpgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG16(include_top = False, weights = 'imagenet', input_shape = input_shape, classes = 2) \n",
        "model2= Sequential() \n",
        "model2.add(base_model) \n",
        "model2.add(Flatten()) \n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "XScJ4q4uwt0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.add(Dense(1024,activation=('relu'),input_dim=2))\n",
        "model2.add(Dense(512,activation=('relu'))) \n",
        "model2.add(Dense(256,activation=('relu'))) \n",
        "\n",
        "model2.add(Dense(128,activation=('relu')))\n",
        "#model.add(Dropout(.2))\n",
        "model2.add(Dense(64,activation=('relu')))\n",
        "model2.add(Dense(32,activation=('relu')))\n",
        "model2.add(Dense(8,activation=('relu'))) \n",
        "#model.add(Dropout(.4))\n",
        "model2.add(Dense(2,activation=('softmax'))) \n",
        "\n",
        "#Checking the final model summary\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "wXjgDFELw85y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try with adam optimizer also \n",
        "batch_size= 200\n",
        "epochs=20 \n",
        "learn_rate=.001 \n",
        "sgd=SGD(learning_rate=learn_rate,momentum=.9,nesterov=False) \n",
        "adam=Adam(learning_rate=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) \n",
        "model2.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7NeIpOeIxHCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               min_delta=0,\n",
        "                               patience=2,\n",
        "                               verbose=0, mode='auto')\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 100\n",
        "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_val, Y_val), verbose = 1)"
      ],
      "metadata": {
        "id": "v_wZNfjzjwvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model2.fit(X_train,Y_train, batch_size = 100, epochs = 30, validation_data = (X_val,Y_val), verbose = 1)"
      ],
      "metadata": {
        "id": "BInV-ygqxUy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print('True positive = ', cm[0][0])\n",
        "    print('False positive = ', cm[0][1])\n",
        "    print('False negative = ', cm[1][0])\n",
        "    print('True negative = ', cm[1][1])\n",
        "    print('\\n')\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
        "    plt.ylabel('Actual label', size = 20)\n",
        "    plt.xlabel('Predicted label', size = 20)\n",
        "    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
        "    plt.ylim([2, 0])\n",
        "    plt.show()\n",
        "print_confusion_matrix(Y_val_org, np.argmax(model2.predict(X), axis=-1))\n",
        "\n"
      ],
      "metadata": {
        "id": "JdwXeAoQzqSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "cm = sklearn.metrics.accuracy_score(Y_val_org, np.argmax(model2.predict(X), axis=-1))\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "esiTWCh_1kft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit==0.80.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us0LvT_LNvSz",
        "outputId": "fcf86e98-706b-470b-c593-6bb318fd8fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: streamlit==0.80.0 in /usr/local/lib/python3.7/dist-packages (0.80.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (1.3.5)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (0.20.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (4.2.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (1.5.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (0.10.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (0.8.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (2.8.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (4.2.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (6.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (2.23.0)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (2.1.9)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (3.1.27)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (7.1.2)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (1.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (6.0.1)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (2.1.1)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (0.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit==0.80.0) (1.21.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.80.0) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.80.0) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.80.0) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.80.0) (0.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.80.0) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.80.0) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.80.0) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.80.0) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.80.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==0.80.0) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit==0.80.0) (2022.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==0.80.0) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.80.0) (7.7.0)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.80.0) (6.15.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.80.0) (5.1.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.1.3)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (7.34.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (5.4.8)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (1.5.5)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (7.3.4)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (23.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (3.0.30)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.7.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (5.4.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (1.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==0.80.0) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (2.15.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.80.0) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (0.13.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit==0.80.0) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.80.0) (5.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.80.0) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit==0.80.0) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.80.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.80.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.80.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==0.80.0) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==4.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q9UeIDAN6Dz",
        "outputId": "49f29087-fe93-408e-f7cf-658f49b8fbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15983 sha256=a1b03e7e54cfa12236652751956ff1f2ba3db700c2edd3858a05be83c9e0c744\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile deepfake_detection_webapp.py \n",
        "\n",
        "import streamlit as str\n",
        "str.header(\"DEEPFAKE DETECTION\")\n",
        "model=tf.keras.models.load_model(\"deepfake-detection-model.h5\")\n",
        "user_input=str.write(\"PLease upload your file here\")            //user input\n",
        "upload_file=str.uploaded_file(\"Drag and Drop\",type=\"png\")     \n",
        "  \n",
        "  predicted_output=model.predict(upload_file)          \n",
        " if str.button('predict'):                 //creating a button\n",
        "     if (predicted_output[0]==0):\n",
        "         str.title(\"Real\")\n",
        "    else:\n",
        "         str.title(\"Fake\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwQqv_4ZOd4Y",
        "outputId": "2761bb2a-c80d-4a57-9744-d75e88da05f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deepfake_detection_webapp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "D2lfQ1C7Ol2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run deepfake_detection_webapp.py & # running streamlit \n",
        "public_url = ngrok.connect(port = '8501') # Creating public URL\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JubzjOzPOuMg",
        "outputId": "c8355094-9d4d-4c63-9209-182c82a4d6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://aa7e-34-143-149-159.ngrok.io'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ny2_1TcmO7vo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}